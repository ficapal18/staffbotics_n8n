
--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/index.js
TOTAL LINES: 35
FILE NAME: index.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/index.js
const { buildRawItemsFromWebhookBody, buildHeuristicAnalysis } = require("./ingestion");
const { autoGroup, extractIdentifiersFromRow } = require("./grouping");
const { applyOperations } = require("./operations");
const { summarizeProposal } = require("./summary");

// (Optional) expose identity helpers for debugging/testing
const {
  normText,
  normId,
  normDate,
  buildPatientKey,
  filenameContainsId,
  filenameMatchesName,
  tokens
} = require("./identity");

module.exports = {
  // pipeline
  buildRawItemsFromWebhookBody,
  buildHeuristicAnalysis,
  autoGroup,
  applyOperations,
  summarizeProposal,

  // optional helpers
  extractIdentifiersFromRow,
  normText,
  normId,
  normDate,
  buildPatientKey,
  filenameContainsId,
  filenameMatchesName,
  tokens
};

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/ingestion/rawItems.js
TOTAL LINES: 74
FILE NAME: rawItems.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/rawItems.js
// LLM usage: buildRawItemsFromWebhookBody(body) -> { rawItems, originalBody }
/**
 * Build rawItems[] from a generic webhook body.
 *
 * Expected body structure (can be adapted):
 * {
 *   excelFiles: [
 *     { name: "patients.xlsx", rows: [ { col1: ..., col2: ... }, ... ] }
 *   ],
 *   files: [
 *     { path: "folderA/1234_report.pdf", filename: "1234_report.pdf" },
 *     ...
 *   ]
 * }
 *
 * Returns: { rawItems, originalBody }
 */
function buildRawItemsFromWebhookBody(body = {}) {
    const excelFiles = body.excelFiles || [];
    const files = body.files || [];
  
    const rawItems = [];
  
    // Excel rows → RawItems
    excelFiles.forEach((ef, efIndex) => {
      const fileName = ef.name || `excel_${efIndex}`;
      const rows = ef.rows || [];
      rows.forEach((row, rowIndex) => {
        rawItems.push({
          id: `${fileName}_row_${rowIndex}`,
          source_type: "excel_row",
          source_ref: `${fileName}#row_${rowIndex}`,
          metadata: {
            file: fileName,
            row_index: rowIndex,
            columns: row
          }
        });
      });
    });
  
    // Generic files → RawItems
    files.forEach((f, fIndex) => {
      const path = f.path || f.fullPath || f.filename;
      const filename = f.filename || (path ? path.split("/").slice(-1)[0] : `file_${fIndex}`);
      let folderPath = null;
      if (path && path.includes("/")) {
        folderPath = path.split("/").slice(0, -1).join("/");
      }
      rawItems.push({
        id: `file_${fIndex}`,
        source_type: "file",
        source_ref: path || filename,
        metadata: {
          filename,
          folder_path: folderPath,
          extension: filename.includes(".")
            ? filename.split(".").slice(-1)[0].toLowerCase()
            : null
        }
      });
    });
  
    return {
      rawItems,
      originalBody: body
    };
  }
  
  module.exports = {
    buildRawItemsFromWebhookBody
  };
  
--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/ingestion/heuristics.js
TOTAL LINES: 109
FILE NAME: heuristics.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/heuristics.js
function buildHeuristicAnalysis(originalBody = {}) {
  const excelFiles = originalBody.excelFiles || [];
  const files = originalBody.files || [];

  const analysisLines = [];

  const blacklist = new Set([
    "tumor_id", "lesion_id", "sample_id", "biopsy_id",
    "study_id", "trial_id", "center_id", "visit_id", "episode_id"
  ]);

  function idLikelihood(colName) {
    const lk = String(colName).toLowerCase();
    if (blacklist.has(lk)) return -10;

    let score = 0;
    if (lk === "nhc" || lk.includes("nhc")) score += 6;
    if (lk.includes("historia") || lk.includes("hc")) score += 4;
    if (lk.includes("patient") && lk.includes("id")) score += 6;
    if (lk.includes("id_paciente") || lk.includes("idpaciente")) score += 6;
    if (lk === "patient_id") score += 8;

    if (lk === "id") score += 1;
    if (lk.endsWith("_id")) score += 0; // neutral now (was dangerous)

    if (lk.includes("tumor") || lk.includes("lesion") || lk.includes("sample")) score -= 6;
    if (lk.includes("study") || lk.includes("trial") || lk.includes("center")) score -= 6;

    return score;
  }

  // Excel analysis
  excelFiles.forEach((ef, efIndex) => {
    const fileName = ef.name || `excel_${efIndex}`;
    const rows = ef.rows || [];
    analysisLines.push(`Excel file '${fileName}' with ${rows.length} rows.`);

    if (rows.length) {
      const colNames = Object.keys(rows[0]);
      analysisLines.push(`Columns: ${JSON.stringify(colNames)}`);

      const colStats = colNames.map((col) => {
        const values = rows.map((r) => String(r[col]));
        const nonNull = values.filter(
          (v) =>
            v !== null &&
            v !== undefined &&
            v !== "" &&
            v !== "null" &&
            v !== "None"
        );
        const distinct = new Set(nonNull);
        const uniqueness = nonNull.length ? distinct.size / nonNull.length : 0.0;
        const nullRatio = values.length ? 1.0 - nonNull.length / values.length : 0.0;

        const like = idLikelihood(col);
        return { name: col, uniqueness, nullRatio, like };
      });

      colStats.sort((a, b) => {
        // prioritize id-likeness, then uniqueness, then null ratio
        if (b.like !== a.like) return b.like - a.like;
        if (b.uniqueness !== a.uniqueness) return b.uniqueness - a.uniqueness;
        return a.nullRatio - b.nullRatio;
      });

      if (colStats.length) {
        const best = colStats[0];
        analysisLines.push(
          `Best ID-like column candidate: '${best.name}' (id_likeness=${best.like}, uniqueness=${best.uniqueness.toFixed(
            2
          )}, null_ratio=${best.nullRatio.toFixed(2)}).`
        );
      }
    }
  });

  // Folder structure analysis
  const folderCounts = {};
  files.forEach((f) => {
    const path = f.path || f.fullPath || f.filename;
    if (!path) return;
    const parts = path.split("/");
    if (parts.length > 1) {
      const folder = parts.slice(0, -1).join("/");
      folderCounts[folder] = (folderCounts[folder] || 0) + 1;
    }
  });

  if (Object.keys(folderCounts).length) {
    analysisLines.push("Folder structure:");
    Object.entries(folderCounts)
      .slice(0, 20)
      .forEach(([folder, count]) => {
        analysisLines.push(` - Folder '${folder}' has ${count} files.`);
      });
  }

  const analysisText = analysisLines.length
    ? analysisLines.join("\n")
    : "No strong structure detected.";

  return { analysisText };
}

module.exports = {
  buildHeuristicAnalysis
};

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/ingestion/index.js
TOTAL LINES: 8
FILE NAME: index.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/ingestion/index.js
const { buildRawItemsFromWebhookBody } = require("./rawItems");
const { buildHeuristicAnalysis } = require("./heuristics");

module.exports = {
  buildRawItemsFromWebhookBody,
  buildHeuristicAnalysis
};

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/identity/normalize.js
TOTAL LINES: 48
FILE NAME: normalize.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/normalize.js
const crypto = require("crypto");

function stripDiacritics(s) {
  return (s || "").normalize("NFD").replace(/[\u0300-\u036f]/g, "");
}

function normText(s) {
  return stripDiacritics(String(s ?? ""))
    .trim()
    .toLowerCase()
    .replace(/\s+/g, " ");
}

function normId(s) {
  // normalize ids: remove spaces, keep alnum and basic separators
  const t = normText(s);
  return t.replace(/[^a-z0-9_-]/g, "");
}

function normDate(s) {
  // extremely defensive normalization: keep digits and separators; do not parse fully here
  const t = normText(s);
  if (!t) return "";
  return t.replace(/[^0-9/-]/g, "");
}

function sha1(s) {
  return crypto.createHash("sha1").update(String(s)).digest("hex");
}

function buildPatientKey(ident = {}) {
  // Priority: patient_id > (name+dob) > name > fallback empty
  const pid = normId(ident.patient_id || "");
  const name = normText(ident.name || "");
  const dob = normDate(ident.dob || "");
  if (pid) return { patient_key: sha1(`pid:${pid}`), match_key_type: "patient_id" };
  if (name && dob) return { patient_key: sha1(`name_dob:${name}|${dob}`), match_key_type: "name_dob" };
  if (name) return { patient_key: sha1(`name:${name}`), match_key_type: "name" };
  return { patient_key: sha1(`unknown:${Math.random()}`), match_key_type: "unknown" };
}

module.exports = {
  normText,
  normId,
  normDate,
  buildPatientKey,
};

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/identity/index.js
TOTAL LINES: 13
FILE NAME: index.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/identity/index.js
const { normText, normId, normDate, buildPatientKey } = require("./normalize");
const { filenameContainsId, filenameMatchesName, tokens } = require("./matching");

module.exports = {
  normText,
  normId,
  normDate,
  buildPatientKey,
  filenameContainsId,
  filenameMatchesName,
  tokens
};

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/identity/matching.js
TOTAL LINES: 64
FILE NAME: matching.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/matching.js
const { normText, normId } = require("./normalize");

/**
 * Tokenize filename-like strings into safe tokens (alnum runs).
 */
function tokens(s) {
  const t = normText(s);
  if (!t) return [];
  return t.split(/[^a-z0-9]+/g).filter(Boolean);
}

/**
 * Boundary-safe check: does normalized filename contain id as a whole token
 * OR as a boundary-delimited substring (non-alnum around it).
 */
function filenameContainsId(filename, patientId) {
  const fid = normText(filename);
  const pid = normId(patientId);
  if (!pid) return false;

  const toks = tokens(fid);
  if (toks.includes(pid)) return true;

  // Boundary-delimited substring match (avoid 123 matching 1234)
  const re = new RegExp(`(^|[^a-z0-9])${pid}([^a-z0-9]|$)`, "i");
  return re.test(fid);
}

/**
 * Stronger name match:
 * - requires at least first token
 * - if last token exists, prefer (first+last) both present
 * - diacritics-insensitive via normalize()
 */
function filenameMatchesName(filename, name) {
  const fnToks = new Set(tokens(filename));
  const n = normText(name);
  if (!n) return false;

  const nameToks = n.split(" ").filter(Boolean);
  if (!nameToks.length) return false;

  const first = nameToks[0];
  const last = nameToks.length > 1 ? nameToks[nameToks.length - 1] : "";

  // Avoid super-common short tokens like "de", "la", "del"
  const stop = new Set(["de", "la", "del", "da", "do", "dos", "das", "van", "von"]);
  const firstOk = first.length >= 3 && !stop.has(first);

  if (!firstOk) return false;

  if (last && last.length >= 3 && !stop.has(last)) {
    return fnToks.has(first) && fnToks.has(last);
  }

  return fnToks.has(first);
}

module.exports = {
  filenameContainsId,
  filenameMatchesName,
  tokens,
};

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/grouping/grouping.js
TOTAL LINES: 402
FILE NAME: grouping.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/grouping.js
// LLM usage: autoGroup(rawItems, groupingConfig, analysisText) -> { patientCandidates, groupingConfig, analysisText, rawItems }

const { buildPatientKey, normId, normText } = require("../identity/normalize");
const { filenameContainsId, filenameMatchesName } = require("../identity/matching");

/**
 * Helper: create a base PatientCandidate object.
 */
function createCandidate(candidateId, inferredKey) {
  return {
    candidate_id: candidateId,
    inferred_key: inferredKey,
    inferred_identifiers: {},
    normalized_identifiers: {},
    patient_key: null,
    match_key_type: null,
    raw_items: [],
    confidence: 0.8,
    status: "ready", // 'ready' | 'quarantine'
    issues: [],
    notes: []
  };
}

/**
 * Scored identifier extraction to avoid picking tumor_id/study_id/etc.
 */
function extractIdentifiersFromRow(row = {}) {
  const keys = Object.keys(row || {});
  const scored = [];

  const blacklisted = [
    "study_id", "trial_id", "center_id", "hospital_id",
    "tumor_id", "lesion_id", "biopsy_id", "sample_id",
    "visit_id", "episode_id", "treatment_id"
  ];

  function scoreKey(lk) {
    let score = 0;

    if (blacklisted.includes(lk)) return -100;

    // strong patient id hints
    if (lk === "nhc" || lk.includes("nhc")) score += 10;
    if (lk.includes("historia") || lk.includes("hc")) score += 6;
    if (lk.includes("patient") && lk.includes("id")) score += 10;
    if (lk.includes("id_paciente") || lk.includes("idpaciente")) score += 10;
    if (lk === "patient_id") score += 12;

    // weak generic "id"
    if (lk === "id") score += 3;
    if (lk.endsWith("_id")) score += 1;

    // punish obvious non-patient id contexts
    if (lk.includes("tumor") || lk.includes("lesion") || lk.includes("sample")) score -= 8;
    if (lk.includes("study") || lk.includes("trial") || lk.includes("center")) score -= 8;

    return score;
  }

  // patient_id candidate
  keys.forEach((k) => {
    const lk = String(k).toLowerCase().trim();
    scored.push({ k, lk, score: scoreKey(lk), value: row[k] });
  });

  scored.sort((a, b) => b.score - a.score);

  const ident = {};
  const bestPid = scored.find(x => x.score >= 6 && x.value !== undefined && x.value !== null && String(x.value).trim() !== "");
  if (bestPid) ident.patient_id = bestPid.value;

  // name
  for (const k of keys) {
    const lk = String(k).toLowerCase();
    if (lk.includes("name") || lk.includes("nom") || lk.includes("cognom") || lk.includes("apellido")) {
      const v = row[k];
      if (v !== undefined && v !== null && String(v).trim() !== "") {
        ident.name = v;
        break;
      }
    }
  }

  // dob
  for (const k of keys) {
    const lk = String(k).toLowerCase();
    if (lk.includes("birth") || lk.includes("dob") || lk.includes("naixement") || lk.includes("fecha_nacimiento")) {
      const v = row[k];
      if (v !== undefined && v !== null && String(v).trim() !== "") {
        ident.dob = v;
        break;
      }
    }
  }

  return ident;
}

/**
 * Normalize identifiers for consistent matching/hashing.
 */
function normalizeIdentifiers(ident = {}) {
  return {
    patient_id: normId(ident.patient_id || ""),
    name: normText(ident.name || ""),
    dob: String(ident.dob ?? "").trim()
  };
}

/**
 * Extract ID from filename using patterns.
 * patterns can be:
 * - string regex  (backward compatible) -> uses match[0]
 * - { pattern: string, group?: number } -> uses match[group||0]
 */
function extractIdFromFilename(filename = "", patterns = []) {
  const reCache = new Map();

  function getRe(pat) {
    if (reCache.has(pat)) return reCache.get(pat);
    try {
      const re = new RegExp(pat, "i");
      reCache.set(pat, re);
      return re;
    } catch (e) {
      reCache.set(pat, null);
      return null;
    }
  }

  for (const p of patterns) {
    if (!p) continue;

    let pat = p;
    let group = 0;
    if (typeof p === "object") {
      pat = p.pattern;
      group = Number.isInteger(p.group) ? p.group : 0;
    }

    if (typeof pat !== "string" || !pat) continue;
    const re = getRe(pat);
    if (!re) continue;

    const m = String(filename).match(re);
    if (m) {
      const g = m[group] ?? m[0];
      const cleaned = normId(g);
      if (cleaned) return cleaned;
    }
  }

  return null;
}

/**
 * Quarantine rules:
 * - confidence < threshold
 * - missing both patient_id and (name+dob)
 * - “id-like” collisions / suspicious ids
 */
function finalizeCandidate(pc, cfg = {}) {
  const qThresh = cfg.quarantine_threshold ?? 0.55;

  const ni = pc.normalized_identifiers || {};
  const hasPid = !!ni.patient_id;
  const hasNameDob = !!(ni.name && ni.dob);

  if (!hasPid && !hasNameDob) {
    pc.issues.push("No reliable identifiers (needs patient_id or name+dob).");
    pc.confidence = Math.min(pc.confidence, 0.35);
  }

  // suspicious short numeric IDs (common source of false matches)
  if (hasPid && /^\d+$/.test(ni.patient_id) && ni.patient_id.length < 4) {
    pc.issues.push("Patient ID looks too short; high collision risk.");
    pc.confidence = Math.min(pc.confidence, 0.45);
  }

  if ((pc.confidence ?? 0) < qThresh) {
    pc.status = "quarantine";
  }

  return pc;
}

/**
 * Main grouping function.
 */
function autoGroup(rawItems = [], groupingConfig = {}, analysisText = "") {
  const unitStrategy = groupingConfig.unit_strategy || "excel_row";
  const excelCfg = groupingConfig.excel || {};
  const folderCfg = groupingConfig.folder || {};
  const quarantineCfg = groupingConfig.quarantine || {};

  const patientCandidates = [];

  // Strategy: excel_row (now: group rows by patient key, not one row == one patient)
  if (unitStrategy === "excel_row") {
    const idColumn = excelCfg.id_column || null;

    // 1) Build candidates from excel rows but GROUP them
    const grouped = new Map(); // key -> candidate

    rawItems
      .filter((ri) => ri.source_type === "excel_row")
      .forEach((ri) => {
        const meta = ri.metadata || {};
        const file = meta.file || "unknown_excel";
        const rowIndex = meta.row_index ?? 0;
        const row = meta.columns || {};

        const identifiers = extractIdentifiersFromRow(row);
        if (idColumn && row[idColumn] !== undefined && row[idColumn] !== null && String(row[idColumn]).trim() !== "") {
          if (!identifiers.patient_id) identifiers.patient_id = row[idColumn];
        }

        const normIdent = normalizeIdentifiers(identifiers);
        const keyInfo = buildPatientKey(normIdent);
        const patientKey = keyInfo.patient_key;

        const groupKey = `${file}::${patientKey}`; // avoid collisions across different excel files

        if (!grouped.has(groupKey)) {
          const pc = createCandidate(`excel::${file}::${patientKey}`, `Excel patient group in ${file}`);
          pc.inferred_identifiers = identifiers;
          pc.normalized_identifiers = normIdent;
          pc.patient_key = patientKey;
          pc.match_key_type = keyInfo.match_key_type;
          pc.raw_items.push(ri);
          pc.confidence = 0.8;
          grouped.set(groupKey, pc);
        } else {
          const pc = grouped.get(groupKey);
          pc.raw_items.push(ri);

          // merge identifiers if missing
          pc.inferred_identifiers = pc.inferred_identifiers || {};
          if (!pc.inferred_identifiers.patient_id && identifiers.patient_id) pc.inferred_identifiers.patient_id = identifiers.patient_id;
          if (!pc.inferred_identifiers.name && identifiers.name) pc.inferred_identifiers.name = identifiers.name;
          if (!pc.inferred_identifiers.dob && identifiers.dob) pc.inferred_identifiers.dob = identifiers.dob;

          pc.normalized_identifiers = normalizeIdentifiers(pc.inferred_identifiers);
          const newKey = buildPatientKey(pc.normalized_identifiers);
          pc.patient_key = newKey.patient_key;
          pc.match_key_type = newKey.match_key_type;

          // slight confidence boost for multi-row evidence
          pc.confidence = Math.min(0.92, (pc.confidence || 0.8) + 0.03);
        }
      });

    // No excel rows? keep empty for now; files will become file_only
    patientCandidates.push(...Array.from(grouped.values()));

    // 2) Attach file items to candidates using safer matching
    const fileItems = rawItems.filter((ri) => ri.source_type === "file");

    fileItems.forEach((fi) => {
      const meta = fi.metadata || {};
      const filename = meta.filename || "";
      let best = null;
      let bestScore = 0;

      for (const pc of patientCandidates) {
        const ni = pc.normalized_identifiers || {};
        const pid = ni.patient_id || "";
        const name = pc.inferred_identifiers?.name || "";

        let score = 0;

        if (pid && filenameContainsId(filename, pid)) score += 10;
        if (name && filenameMatchesName(filename, name)) score += 4;

        if (score > bestScore) {
          bestScore = score;
          best = pc;
        }
      }

      if (best && bestScore >= 6) {
        best.raw_items.push(fi);
        best.confidence = Math.min(0.95, (best.confidence || 0.8) + 0.02);
      } else {
        const pc = createCandidate(`file_only_${fi.id}`, `Unassigned file ${meta.filename || fi.id}`);
        pc.raw_items.push(fi);
        pc.confidence = 0.3;
        pc.status = "quarantine";
        pc.issues.push("Unassigned file; no confident patient match.");
        patientCandidates.push(pc);
      }
    });

    // finalize
    patientCandidates.forEach(pc => finalizeCandidate(pc, quarantineCfg));
  }

  // Strategy: subfolder (now: confidence depends on folder “patient-likeness”)
  else if (unitStrategy === "subfolder") {
    const patterns = folderCfg.id_patterns || [];
    const folderMap = {};

    rawItems
      .filter((ri) => ri.source_type === "file")
      .forEach((ri) => {
        const meta = ri.metadata || {};
        const folder = meta.folder_path || "root";
        if (!folderMap[folder]) folderMap[folder] = [];
        folderMap[folder].push(ri);
      });

    Object.entries(folderMap).forEach(([folder, list]) => {
      const pc = createCandidate(`folder::${folder}`, `Folder '${folder}'`);
      pc.raw_items.push(...list);

      // Extract patient id from folder leaf if possible
      const leaf = String(folder).split("/").slice(-1)[0];
      const extracted = extractIdFromFilename(leaf, patterns);
      if (extracted) {
        pc.inferred_identifiers = { patient_id: extracted };
        pc.normalized_identifiers = normalizeIdentifiers(pc.inferred_identifiers);
        const keyInfo = buildPatientKey(pc.normalized_identifiers);
        pc.patient_key = keyInfo.patient_key;
        pc.match_key_type = keyInfo.match_key_type;
        pc.confidence = 0.85;
      } else {
        // Folder might be batch/date/hospital rather than patient
        pc.confidence = 0.55;
        pc.issues.push("Folder grouping used but folder name does not look patient-specific.");
      }

      finalizeCandidate(pc, quarantineCfg);
      patientCandidates.push(pc);
    });
  }

  // Strategy: id_in_filename (now: supports capture groups)
  else if (unitStrategy === "id_in_filename") {
    const patterns = folderCfg.id_patterns || [];
    const idMap = {};

    rawItems
      .filter((ri) => ri.source_type === "file")
      .forEach((ri) => {
        const meta = ri.metadata || {};
        const filename = meta.filename || "";
        const matchedId = extractIdFromFilename(filename, patterns);

        const key = matchedId ? `id::${matchedId}` : `unassigned::${ri.id}`;
        if (!idMap[key]) idMap[key] = [];
        idMap[key].push(ri);
      });

    Object.entries(idMap).forEach(([key, list]) => {
      const pc = createCandidate(key, `ID grouping '${key}'`);
      pc.raw_items.push(...list);

      if (key.startsWith("id::")) {
        const pid = key.replace(/^id::/, "");
        pc.inferred_identifiers = { patient_id: pid };
        pc.normalized_identifiers = normalizeIdentifiers(pc.inferred_identifiers);
        const keyInfo = buildPatientKey(pc.normalized_identifiers);
        pc.patient_key = keyInfo.patient_key;
        pc.match_key_type = keyInfo.match_key_type;
        pc.confidence = 0.82;
      } else {
        pc.confidence = 0.3;
        pc.status = "quarantine";
        pc.issues.push("Unassigned file; no ID pattern match.");
      }

      finalizeCandidate(pc, quarantineCfg);
      patientCandidates.push(pc);
    });
  }

  // Strategy: fallback
  else {
    rawItems.forEach((ri) => {
      const pc = createCandidate(`single::${ri.id}`, `Single source item ${ri.id}`);
      pc.raw_items.push(ri);
      pc.confidence = 0.2;
      pc.status = "quarantine";
      pc.issues.push("Fallback grouping: one item per candidate.");
      patientCandidates.push(pc);
    });
  }

  return {
    patientCandidates,
    groupingConfig,
    analysisText,
    rawItems
  };
}

module.exports = {
  autoGroup,
  extractIdentifiersFromRow
};

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/grouping/index.js
TOTAL LINES: 7
FILE NAME: index.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/grouping/index.js
const { autoGroup, extractIdentifiersFromRow } = require("./grouping");

module.exports = {
  autoGroup,
  extractIdentifiersFromRow
};

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/operations/index.js
TOTAL LINES: 6
FILE NAME: index.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/operations/index.js
const { applyOperations } = require("./operations");

module.exports = {
  applyOperations
};

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/operations/operations.js
TOTAL LINES: 88
FILE NAME: operations.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/operations.js
// LLM usage: applyOperations(patientCandidates, operations, userInstruction) -> { patientCandidates, operationsApplied, userInstruction }
/**
 * Apply a list of operations (merge, reassign, change_strategy) over patientCandidates.
 *
 * @param {Array} patientCandidates
 * @param {Array} operations - list of { op: string, params: any }
 * @param {string} userInstruction
 *
 * Returns: { patientCandidates, operationsApplied, userInstruction }
 */
function applyOperations(patientCandidates = [], operations = [], userInstruction = "") {
    const pcById = {};
    patientCandidates.forEach((pc) => {
      pcById[pc.candidate_id] = pc;
    });
  
    function applyMerge(params) {
      const from = params.from || [];
      const into = params.into;
      if (!into || !pcById[into]) return;
      const dst = pcById[into];
  
      from.forEach((sid) => {
        if (sid === into) return;
        const src = pcById[sid];
        if (!src) return;
        const srcItems = src.raw_items || [];
        dst.raw_items = (dst.raw_items || []).concat(srcItems);
        src.raw_items = [];
        src.notes = (src.notes || []).concat(`Merged into ${into}`);
        src.confidence = 0.0;
      });
    }
  
    function applyReassign(params) {
      const rid = params.raw_item_id;
      const newCid = params.new_candidate;
      if (!rid || !pcById[newCid]) return;
  
      // Remove from all candidates
      patientCandidates.forEach((pc) => {
        const ris = pc.raw_items || [];
        pc.raw_items = ris.filter((ri) => ri.id !== rid);
      });
  
      // We assume the rawItem still exists globally and could be looked up.
      // Here, we push a stub to mark that it was reassigned.
      const target = pcById[newCid];
      target.raw_items = target.raw_items || [];
      target.raw_items.push({
        id: rid,
        source_type: "unknown",
        source_ref: "reassigned",
        metadata: {}
      });
    }
  
    function applyChangeStrategy(params) {
      const newStrategy = params.unit_strategy;
      if (!newStrategy) return;
      patientCandidates.forEach((pc) => {
        pc.notes = pc.notes || [];
        pc.notes.push(
          `User requested strategy change to ${newStrategy} (not auto-applied in this step).`
        );
      });
    }
  
    operations.forEach((op) => {
      const opType = op.op;
      const params = op.params || {};
      if (opType === "merge") applyMerge(params);
      else if (opType === "reassign") applyReassign(params);
      else if (opType === "change_strategy") applyChangeStrategy(params);
    });
  
    return {
      patientCandidates,
      operationsApplied: operations,
      userInstruction
    };
  }
  
  module.exports = {
    applyOperations
  };
  
--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/summary/index.js
TOTAL LINES: 6
FILE NAME: index.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/summary/index.js
const { summarizeProposal } = require("./summary");

module.exports = {
  summarizeProposal
};

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/src/summary/summary.js
TOTAL LINES: 35
FILE NAME: summary.js
FILE EXTENSION: .js
--------------------------------------------------------------------------------
// src/summary.js
function summarizeProposal(patientCandidates = [], groupingConfig = {}, analysisText = "") {
  const summaryLines = [];

  summaryLines.push("Proposed patient grouping structure:");
  summaryLines.push(`- Strategy: ${groupingConfig.unit_strategy}`);
  summaryLines.push(`- Excel config: ${JSON.stringify(groupingConfig.excel || {})}`);
  summaryLines.push(`- Folder config: ${JSON.stringify(groupingConfig.folder || {})}`);
  summaryLines.push(`- Number of patient candidates: ${patientCandidates.length}`);

  const lowConf = patientCandidates.filter((pc) => (pc.confidence || 0) < 0.5);
  const quarantine = patientCandidates.filter((pc) => pc.status === "quarantine");
  summaryLines.push(`- Low confidence candidates (<0.5): ${lowConf.length}`);
  summaryLines.push(`- Quarantined candidates: ${quarantine.length}`);

  // Top issues
  const issueCounts = {};
  quarantine.forEach(pc => (pc.issues || []).forEach(i => issueCounts[i] = (issueCounts[i] || 0) + 1));
  const topIssues = Object.entries(issueCounts).sort((a,b)=>b[1]-a[1]).slice(0,5);
  if (topIssues.length) {
    summaryLines.push("- Top quarantine reasons:");
    topIssues.forEach(([k,v]) => summaryLines.push(`  - ${k} (${v})`));
  }

  summaryLines.push("");
  summaryLines.push("Heuristic analysis recap:");
  summaryLines.push(analysisText || "(none)");

  const summary = summaryLines.join("\n");
  return { summary };
}

module.exports = {
  summarizeProposal
};

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/workflows/staffbotics.json
TOTAL LINES: 218
FILE NAME: staffbotics.json
FILE EXTENSION: .json
--------------------------------------------------------------------------------
{
  "name": "Staffbotics Patient Grouping (MVP)",
  "nodes": [
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "staffbotics-batch",
        "responseMode": "lastNode",
        "options": {}
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [260, 300],
      "id": "a1a0f9f2-8e44-4f83-8b2d-2c3e2a5c9c11",
      "name": "Receive Batch (Webhook)",
      "webhookId": "b6f6c9f4-7c8e-4ef2-8c1d-4d1c0b4b8b33"
    },
    {
      "parameters": {
        "functionCode": "const { buildRawItemsFromWebhookBody } = require('staffbotics-helpers');\n\n// Webhook payload is usually in items[0].json.body, but handle both cases.\nconst payload = items[0].json.body ?? items[0].json;\n\nconst out = buildRawItemsFromWebhookBody(payload);\n\nreturn [{ json: out }];"
      },
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [620, 300],
      "id": "c2d7ff1b-1c53-4b9a-9c4b-7d3a3b8b6a10",
      "name": "Build RawItems"
    },
    {
      "parameters": {
        "functionCode": "const { buildHeuristicAnalysis } = require('staffbotics-helpers');\n\nconst originalBody = items[0].json.originalBody ?? {};\nconst { analysisText } = buildHeuristicAnalysis(originalBody);\n\nreturn [{\n  json: {\n    ...items[0].json,\n    analysisText\n  }\n}];"
      },
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [980, 300],
      "id": "aa1b8fd8-7f8e-4d0f-9c6b-6a1c8e60b2cf",
      "name": "Heuristic Analysis"
    },
    {
      "parameters": {
        "url": "https://api.openai.com/v1/chat/completions",
        "method": "POST",
        "authentication": "none",
        "jsonParameters": true,
        "sendHeaders": true,
        "headerParametersJson": "{\n  \"Authorization\": \"Bearer {{$env.OPENAI_API_KEY}}\",\n  \"Content-Type\": \"application/json\"\n}",
        "bodyParametersJson": "{\n  \"model\": \"gpt-5.2-mini\",\n  \"temperature\": 0,\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You output ONLY valid JSON (no markdown, no prose).\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"You are configuring a deterministic bulk-to-patient grouping step.\\n\\nReturn ONLY a JSON object with this schema:\\n{\\n  \\\"unit_strategy\\\": \\\"excel_row\\\"|\\\"subfolder\\\"|\\\"id_in_filename\\\"|\\\"fallback\\\",\\n  \\\"excel\\\": { \\\"id_column\\\": string|null },\\n  \\\"folder\\\": { \\\"id_patterns\\\": (string|{\\\"pattern\\\":string,\\\"group\\\":number})[] },\\n  \\\"quarantine\\\": { \\\"quarantine_threshold\\\": number }\\n}\\n\\nGuidance:\\n- If any Excel is present, prefer unit_strategy=excel_row, and set excel.id_column if an ID-like column is clear.\\n- If folders strongly look like per-patient folders, choose subfolder.\\n- If filenames contain consistent patient IDs, choose id_in_filename and propose 1-3 regex patterns. Prefer capture groups for the ID: {pattern: '...', group: 1}.\\n- Otherwise fallback.\\n- Set quarantine.quarantine_threshold to a sensible default (e.g., 0.55). Use higher (0.65) if data is noisy and you want fewer auto-processed candidates.\\n\\nHeuristic analysis:\\n{{$node[\\\"Heuristic Analysis\\\"].json.analysisText}}\"\n    }\n  ]\n}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [1360, 300],
      "id": "4e6b0d0a-3c31-4f6d-bd8c-7a76c23f7c65",
      "name": "Structure Advisor (LLM)"
    },
    {
      "parameters": {
        "functionCode": "function safeParseJson(text) {\n  if (!text || typeof text !== 'string') return null;\n  try { return JSON.parse(text); } catch (e) {}\n  const m = text.match(/\\{[\\s\\S]*\\}/);\n  if (m) {\n    try { return JSON.parse(m[0]); } catch (e) {}\n  }\n  return null;\n}\n\n// This node receives the HTTP response as items[0].json\nconst llmResp = items[0].json;\nconst content = llmResp?.choices?.[0]?.message?.content ?? '';\n\nconst groupingConfig = safeParseJson(content) || { unit_strategy: 'excel_row', excel: { id_column: null }, folder: { id_patterns: [] }, quarantine: { quarantine_threshold: 0.55 } };\n\n// Pull rawItems + analysisText from the previous deterministic nodes\nconst rawItems = $node['Heuristic Analysis'].json.rawItems ?? [];\nconst analysisText = $node['Heuristic Analysis'].json.analysisText ?? '';\nconst originalBody = $node['Heuristic Analysis'].json.originalBody ?? {};\n\nreturn [{\n  json: {\n    rawItems,\n    analysisText,\n    originalBody,\n    groupingConfig,\n    llm_raw: content\n  }\n}];"
      },
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [1720, 300],
      "id": "0a9f9e16-78bb-4f46-9b03-2b0d6a0f63f6",
      "name": "Parse LLM Config"
    },
    {
      "parameters": {
        "functionCode": "const { autoGroup } = require('staffbotics-helpers');\n\nconst rawItems = items[0].json.rawItems ?? [];\nconst groupingConfig = items[0].json.groupingConfig ?? { unit_strategy: 'excel_row' };\nconst analysisText = items[0].json.analysisText ?? '';\n\nconst out = autoGroup(rawItems, groupingConfig, analysisText);\n\nreturn [{ json: out }];"
      },
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [2080, 300],
      "id": "d8b0f2aa-4a6a-4d65-a7c4-7f6b8b3f6d90",
      "name": "Auto Grouping"
    },
    {
      "parameters": {
        "functionCode": "const { summarizeProposal } = require('staffbotics-helpers');\n\nconst patientCandidates = items[0].json.patientCandidates ?? [];\nconst groupingConfig = items[0].json.groupingConfig ?? {};\nconst analysisText = items[0].json.analysisText ?? '';\n\nconst { summary } = summarizeProposal(patientCandidates, groupingConfig, analysisText);\n\nreturn [{\n  json: {\n    ...items[0].json,\n    summary\n  }\n}];"
      },
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [2440, 300],
      "id": "8a2f2c2e-6b3d-4c7d-8c0b-3d1d92f3d2aa",
      "name": "Summarize Proposal"
    },
    {
      "parameters": {
        "httpMethod": "POST",
        "path": "staffbotics-reorg",
        "responseMode": "lastNode",
        "options": {}
      },
      "type": "n8n-nodes-base.webhook",
      "typeVersion": 2.1,
      "position": [260, 760],
      "id": "b8d2f41c-fd8f-4c72-9f22-1d2ed1f5b9d1",
      "name": "Apply Reorg (Webhook)",
      "webhookId": "3d5d1d1a-3f1b-4db5-9c7c-83a5e9b2a2fd"
    },
    {
      "parameters": {
        "url": "https://api.openai.com/v1/chat/completions",
        "method": "POST",
        "authentication": "none",
        "jsonParameters": true,
        "sendHeaders": true,
        "headerParametersJson": "{\n  \"Authorization\": \"Bearer {{$env.OPENAI_API_KEY}}\",\n  \"Content-Type\": \"application/json\"\n}",
        "bodyParametersJson": "{\n  \"model\": \"gpt-5.2-mini\",\n  \"temperature\": 0,\n  \"messages\": [\n    {\n      \"role\": \"system\",\n      \"content\": \"You output ONLY valid JSON array (no markdown, no prose).\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"Convert the instruction into an operations JSON array with schema: [{op:'merge'|'reassign'|'change_strategy', params:{...}}].\\n\\nInstruction:\\n{{$node[\\\"Apply Reorg (Webhook)\\\"].json.body.instruction}}\\n\\nValid candidate_ids:\\n{{JSON.stringify(($node[\\\"Apply Reorg (Webhook)\\\"].json.body.patientCandidates||[]).map(c=>c.candidate_id))}}\\n\\nValid raw_item ids:\\n{{JSON.stringify(($node[\\\"Apply Reorg (Webhook)\\\"].json.body.patientCandidates||[]).flatMap(c=>(c.raw_items||[]).map(r=>r.id)))}}\\n\\nRules:\\n- Use only valid IDs from the lists.\\n- merge params: {from:[candidate_id...], into:candidate_id}\\n- reassign params: {raw_item_id:rawItemId, new_candidate:candidate_id}\\n- change_strategy params: {unit_strategy:'excel_row'|'subfolder'|'id_in_filename'|'fallback'}\\nReturn JSON array only.\"\n    }\n  ]\n}",
        "options": {}
      },
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 1,
      "position": [620, 760],
      "id": "f4b6e3ad-2c76-4a8f-9f2b-2b3dc5eaf881",
      "name": "NL → Operations (LLM)"
    },
    {
      "parameters": {
        "functionCode": "const { applyOperations } = require('staffbotics-helpers');\n\nfunction safeParseJsonArray(text) {\n  if (!text || typeof text !== 'string') return [];\n  try {\n    const parsed = JSON.parse(text);\n    return Array.isArray(parsed) ? parsed : [];\n  } catch (e) {}\n  const m = text.match(/\\[[\\s\\S]*\\]/);\n  if (m) {\n    try {\n      const parsed = JSON.parse(m[0]);\n      return Array.isArray(parsed) ? parsed : [];\n    } catch (e) {}\n  }\n  return [];\n}\n\n// Reorg webhook payload (the state the user sends in)\nconst payload = $node['Apply Reorg (Webhook)'].json.body ?? {};\nconst patientCandidates = payload.patientCandidates ?? [];\nconst rawItems = payload.rawItems ?? [];\nconst userInstruction = payload.instruction ?? '';\n\n// LLM operations response content\nconst llmResp = items[0].json;\nconst content = llmResp?.choices?.[0]?.message?.content ?? '';\nconst operations = safeParseJsonArray(content);\n\n// Build lookup for rawItems to repair reassign stubs\nconst rawItemById = {};\nfor (const ri of rawItems) rawItemById[ri.id] = ri;\n\nconst out = applyOperations(patientCandidates, operations, userInstruction);\n\n// Repair stubs produced by applyOperations(reassign)\nfor (const pc of out.patientCandidates || []) {\n  pc.raw_items = (pc.raw_items || []).map(ri => {\n    if (ri && ri.source_ref === 'reassigned' && rawItemById[ri.id]) return rawItemById[ri.id];\n    return ri;\n  });\n}\n\nreturn [{\n  json: {\n    patientCandidates: out.patientCandidates,\n    operationsApplied: out.operationsApplied,\n    userInstruction,\n    rawItems,\n    llm_raw: content\n  }\n}];"
      },
      "type": "n8n-nodes-base.function",
      "typeVersion": 1,
      "position": [980, 760],
      "id": "3b8b7a5e-1e2a-4c55-8d77-6b4f8c7b3c21",
      "name": "Apply Operations"
    }
  ],
  "connections": {
    "Receive Batch (Webhook)": {
      "main": [
        [
          {
            "node": "Build RawItems",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build RawItems": {
      "main": [
        [
          {
            "node": "Heuristic Analysis",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Heuristic Analysis": {
      "main": [
        [
          {
            "node": "Structure Advisor (LLM)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Structure Advisor (LLM)": {
      "main": [
        [
          {
            "node": "Parse LLM Config",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse LLM Config": {
      "main": [
        [
          {
            "node": "Auto Grouping",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Auto Grouping": {
      "main": [
        [
          {
            "node": "Summarize Proposal",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Apply Reorg (Webhook)": {
      "main": [
        [
          {
            "node": "NL → Operations (LLM)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "NL → Operations (LLM)": {
      "main": [
        [
          {
            "node": "Apply Operations",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "settings": {}
}

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/data/example_payload.json
TOTAL LINES: 17
FILE NAME: example_payload.json
FILE EXTENSION: .json
--------------------------------------------------------------------------------
{
    "excelFiles": [
      {
        "name": "patients.xlsx",
        "rows": [
          { "Name": "John Doe", "NHC": "1234", "Age": 59, "Comments": "Has 2 PDF documents" },
          { "Name": "Anna Smith", "NHC": "1235", "Age": 71, "Comments": "Only one report file" }
        ]
      }
    ],
    "files": [
      { "path": "sample_batch_1/1234_scan.pdf", "filename": "1234_scan.pdf" },
      { "path": "sample_batch_1/1234_report.pdf", "filename": "1234_report.pdf" },
      { "path": "sample_batch_1/1235_report.pdf", "filename": "1235_report.pdf" }
    ]
  }
  
--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/data/sample_batch_1/1235_report.pdf
TOTAL LINES: 1
FILE NAME: 1235_report.pdf
FILE EXTENSION: .pdf
--------------------------------------------------------------------------------
Dummy report for 1235

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/data/sample_batch_1/1234_scan.pdf
TOTAL LINES: 1
FILE NAME: 1234_scan.pdf
FILE EXTENSION: .pdf
--------------------------------------------------------------------------------
Dummy scan for 1234

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/data/sample_batch_1/1234_report.pdf
TOTAL LINES: 1
FILE NAME: 1234_report.pdf
FILE EXTENSION: .pdf
--------------------------------------------------------------------------------
Dummy report for 1234

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/docker-compose.yml
TOTAL LINES: 191
FILE NAME: docker-compose.yml
FILE EXTENSION: .yml
--------------------------------------------------------------------------------
# docker-compose.yml
#
# Goal:
# - Portable n8n + Postgres (state in volumes)
# - Converge on every boot to:
#   - DB schema migrated
#   - workflow imported from ./workflows/staffbotics.json (if missing)
#   - exactly one workflow row with WF_NAME is active=true
#   - production webhooks registered and responding on /webhook/*
#
# Why 3 n8n-related services (init + server + poststart toggle)?
# - n8n-init (one-shot): ensures DB is ready + workflow exists + DB active flag is correct
# - n8n (long-running): registers production webhook routes at runtime from active workflows
# - n8n-poststart-toggle (one-shot): forces a UI-like "toggle active" after server is up,
#   which can re-register webhook routes in cases where a workflow is active in DB
#   but /webhook/* still returns "not registered".
#
# Files expected in repo:
# - ./workflows/staffbotics.json
# - ./scripts/n8n_init.sh
# - ./scripts/n8n_poststart_toggle.sh
#
# Notes:
# - Compose loads .env automatically from the same folder (default behavior).
# - Keep N8N_ENCRYPTION_KEY stable across machines, otherwise credentials break.
# - If you enable Basic Auth, the poststart-toggle MUST send -u user:pass to /rest/*.

services:
  # -----------------------------
  # Postgres (persistent state)
  # -----------------------------
  db:
    image: postgres:14
    container_name: staffbotics_db
    restart: unless-stopped

    volumes:
      - db_data:/var/lib/postgresql/data

    environment:
      # From your .env (fallbacks are safe for local dev)
      POSTGRES_DB: ${DB_POSTGRESDB_DATABASE:-n8n}
      POSTGRES_USER: ${DB_POSTGRESDB_USER:-n8n}
      POSTGRES_PASSWORD: ${DB_POSTGRESDB_PASSWORD:-n8n}

    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_POSTGRESDB_USER:-n8n} -d ${DB_POSTGRESDB_DATABASE:-n8n}"]
      interval: 3s
      timeout: 3s
      retries: 40

  # ----------------------------------------------------------
  # n8n-init (one-shot): migrations + import + DB active flag
  # ----------------------------------------------------------
  n8n-init:
    build:
      context: .
      dockerfile: Dockerfile
    image: staffbotics_n8n_local:latest
    container_name: staffbotics_n8n_init
    restart: "no"

    depends_on:
      db:
        condition: service_healthy

    # We run a deterministic shell script instead of the default n8n entrypoint.
    entrypoint: ["/bin/sh", "-lc"]
    command: ["/data/scripts/n8n_init.sh"]

    environment:
      # --- n8n DB config (must match what the main n8n server will use) ---
      DB_TYPE: ${DB_TYPE:-postgresdb}
      DB_POSTGRESDB_HOST: ${DB_POSTGRESDB_HOST:-db}
      DB_POSTGRESDB_PORT: ${DB_POSTGRESDB_PORT:-5432}
      DB_POSTGRESDB_DATABASE: ${DB_POSTGRESDB_DATABASE:-n8n}
      DB_POSTGRESDB_USER: ${DB_POSTGRESDB_USER:-n8n}
      DB_POSTGRESDB_PASSWORD: ${DB_POSTGRESDB_PASSWORD:-n8n}

      # --- required for portability of credentials ---
      N8N_ENCRYPTION_KEY: ${N8N_ENCRYPTION_KEY}

      # --- workflow import/activation settings ---
      WF_NAME: ${WF_NAME}
      WF_JSON_PATH: /data/workflows/staffbotics.json

      # --- used only for the temporary bootstrap start inside init ---
      N8N_PORT: ${N8N_PORT:-5678}

  # -----------------------------------
  # n8n (main server / long-running)
  # -----------------------------------
  n8n:
    build:
      context: .
      dockerfile: Dockerfile
    image: staffbotics_n8n_local:latest
    container_name: staffbotics_n8n
    restart: unless-stopped

    depends_on:
      db:
        condition: service_healthy
      n8n-init:
        condition: service_completed_successfully

    ports:
      - "${N8N_PORT:-5678}:5678"

    volumes:
      # n8n user folder (contains settings, encryption key usage, etc.)
      - n8n_data:/home/node/.n8n

    environment:
      # --- n8n DB config ---
      DB_TYPE: ${DB_TYPE:-postgresdb}
      DB_POSTGRESDB_HOST: ${DB_POSTGRESDB_HOST:-db}
      DB_POSTGRESDB_PORT: ${DB_POSTGRESDB_PORT:-5432}
      DB_POSTGRESDB_DATABASE: ${DB_POSTGRESDB_DATABASE:-n8n}
      DB_POSTGRESDB_USER: ${DB_POSTGRESDB_USER:-n8n}
      DB_POSTGRESDB_PASSWORD: ${DB_POSTGRESDB_PASSWORD:-n8n}

      # --- required for portability of credentials ---
      N8N_ENCRYPTION_KEY: ${N8N_ENCRYPTION_KEY}

      # --- server settings ---
      N8N_HOST: ${N8N_HOST:-localhost}
      N8N_PORT: ${N8N_PORT:-5678}
      N8N_PROTOCOL: ${N8N_PROTOCOL:-http}
      N8N_EDITOR_BASE_URL: ${N8N_EDITOR_BASE_URL:-http://localhost:5678}

      # --- auth settings (optional, but you have them enabled) ---
      N8N_BASIC_AUTH_USER: ${N8N_BASIC_AUTH_USER}
      N8N_BASIC_AUTH_PASSWORD: ${N8N_BASIC_AUTH_PASSWORD}

      # If you truly want "single-user" style setups, this is common.
      # (If you use user management instead, adjust accordingly.)
      N8N_USER_MANAGEMENT_DISABLED: ${N8N_USER_MANAGEMENT_DISABLED:-true}
      N8N_USER_EMAIL: ${N8N_USER_EMAIL:-admin@admin.com}

      # --- optional logging/config ---
      N8N_LOG_LEVEL: ${N8N_LOG_LEVEL:-info}

      # Allow require() in Function/Code nodes (dev convenience)
      NODE_FUNCTION_ALLOW_EXTERNAL: ${NODE_FUNCTION_ALLOW_EXTERNAL:-*}
      NODE_FUNCTION_ALLOW_BUILTIN: ${NODE_FUNCTION_ALLOW_BUILTIN:-*}

  # -------------------------------------------------------------------
  # n8n-poststart-toggle (one-shot): force UI-like activate toggle
  # -------------------------------------------------------------------
  n8n-poststart-toggle:
    image: staffbotics_n8n_local:latest
    container_name: staffbotics_n8n_poststart_toggle
    restart: "no"

    # Wait until the main server container has started.
    # (We still wait on /healthz inside the script for readiness.)
    depends_on:
      n8n:
        condition: service_started

    entrypoint: ["/bin/sh", "-lc"]
    command: ["/data/scripts/n8n_poststart_toggle.sh"]

    environment:
      # --- DB lookup for workflow ID by name ---
      DB_POSTGRESDB_HOST: ${DB_POSTGRESDB_HOST:-db}
      DB_POSTGRESDB_PORT: ${DB_POSTGRESDB_PORT:-5432}
      DB_POSTGRESDB_DATABASE: ${DB_POSTGRESDB_DATABASE:-n8n}
      DB_POSTGRESDB_USER: ${DB_POSTGRESDB_USER:-n8n}
      DB_POSTGRESDB_PASSWORD: ${DB_POSTGRESDB_PASSWORD:-n8n}

      # Workflow name to target
      WF_NAME: ${WF_NAME}

      # Internal URL to reach n8n from inside the compose network
      N8N_INTERNAL_URL: http://n8n:5678

      # IMPORTANT:
      # If Basic Auth is enabled on n8n, /rest/* is protected.
      # The script must authenticate when PATCHing /rest/workflows/:id.
      N8N_BASIC_AUTH_USER: ${N8N_BASIC_AUTH_USER}
      N8N_BASIC_AUTH_PASSWORD: ${N8N_BASIC_AUTH_PASSWORD}

      # Optional verification probe (must match your webhook node)
      WEBHOOK_METHOD: POST
      WEBHOOK_PATH: staffbotics-batch

volumes:
  db_data:
  n8n_data:

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/README.md
TOTAL LINES: 329
FILE NAME: README.md
FILE EXTENSION: .md
--------------------------------------------------------------------------------
# Staffbotics Pipeline Dev

## 🔍 Purpose
This repository supports the development of the Staffbotics patient grouping pipeline. The goal is to process raw input data (excel rows, medical documents, folders) to produce structured groupings of patients for Athena Tech survival analysis. This grouping process is purely structural; **no clinical inference or entity recognition** should be done here - that part is already operational in Athena Tech.

The aim: automate ingestion of batch data and files, auto-detect grouping logic, allow human review and adjustment via natural language or UI, and feed structured patient bundles into downstream pipelines.

This pipeline turns messy, bulk clinical inputs (Excel files, PDFs, folders, mixed formats) into clean, per-patient data packages that can be reliably processed by downstream AI models. It first normalizes all inputs into a common internal format, analyzes their structure (rows, folders, filenames), and uses a constrained LLM only to choose a grouping strategy, not to extract data. The actual grouping is deterministic: Excel rows are merged into single patients using stable identity keys, files are safely attached using boundary-aware ID and name matching, and each resulting patient candidate is assigned a confidence score and explicit identity metadata.

Crucially, the pipeline is designed for safety and scale. Ambiguous or low-confidence groupings are automatically quarantined instead of silently propagated, while high-confidence patients are marked “ready” for automated ingestion. A second endpoint allows humans to correct grouping mistakes using natural language, which is translated into deterministic operations (merge, reassign) without breaking reproducibility. The result is a production-ready bulk ingestion layer that removes manual data entry bottlenecks, preserves trust, and prepares each patient package for reliable downstream extraction, survival modeling, and future database reconciliation.

---

## 🚀 Instructions to Use

### 1️⃣ Install Cursor
Download and install Cursor from:  
👉 https://cursor.com/

### 2️⃣ Create the repository folder and files
```bash
mkdir staffbotics_n8n
cd staffbotics_n8n
git init .
```

### 3️⃣ Launch Docker stack
```bash
docker compose up -d
```

### 4️⃣ Open the repository in Cursor
```bash
cursor .
```

### 5️⃣ Edit workflow using Cursor → then re-import into n8n. **From inside n8n container**:

### Ensure the API is enabled
In n8n UI:
Go to Settings → API

Enable API (if needed)
Create a new API Key
Copy it.
B) Put it into your .env
Add:
N8N_API_KEY=PASTE_YOUR_KEY_HERE

```bash
sh /data/scripts/import_overwrite.sh /data/workflows/staffbotics.json
```

### 7️⃣ Test the setup
Use drag-and-drop via a local upload form or send a JSON test payload to the webhook.

### Testing the Patient Grouping Pipeline

To test the bulk patient grouping workflow locally, start the n8n workflow in **Test** mode and use the test webhook endpoint.

1. Open the workflow in n8n and click **Execute Workflow**.
2. Copy the test webhook URL for the `Receive Batch (Webhook)` node  
   (e.g. `http://localhost:5678/webhook-test/staffbotics-batch`).
3. Send a sample payload:

```bash
curl -X POST "http://localhost:5678/webhook/staffbotics-batch" \
  -H "Content-Type: application/json" \
  --data-binary @data/example_payload.json
```

Do not use this one, use the one above
```bash
curl -X POST "http://localhost:5678/webhook-test/staffbotics-batch" \
  -H "Content-Type: application/json" \
  --data-binary @data/example_payload.json
```



If the workflow is working correctly, it will return a grouping proposal with one patient candidate per individual, correctly attached files, confidence scores, and a human-readable summary. Only candidates marked as status: "ready" should be considered safe for downstream processing.

########################################## We can delete from here down




## 📦 Repo Structure

```
/staffbotics-pipeline-dev
  ├─ src/                      # Reusable JavaScript/Python logic for grouping pipeline
  ├─ workflows/               # Exported n8n workflows (.json format)
  ├─ data/                    # Sample datasets for testing batch uploads
  ├─ db/                      # Postgres volume
  ├─ docker-compose.yml       # Local dev stack with n8n + PostgreSQL
  ├─ .env                     # Environment variables locally
  └─ README.md                # You are here



Ingress (ingestion) → Inspect (ingestion) → Decide (grouping) → Group (grouping) → Repair (operations) → Summarize (summary) → Fan-out later (execution)


src/
├── ingestion/
│   ├── rawItems.js          # Webhook payload → RawItem[]
│   ├── heuristics.js        # Structure inspection (Excel + folders)
│   └── index.js
│
├── identity/
│   ├── normalize.js         # Canonical normalization
│   ├── matching.js          # Cross-source matching
│   ├── patientKey.js        # Identity key logic (optional split)
│   └── index.js
│
├── grouping/
│   ├── grouping.js          # autoGroup (strategy orchestration)
│   ├── quarantine.js        # Rules & thresholds (can start inline)
│   └── index.js
│
├── operations/
│   ├── operations.js        # merge / reassign / strategy ops
│   └── index.js
│
├── summary/
│   ├── summary.js           # Human-readable output
│   └── index.js
│
└── index.js                 # Public API for n8n





```




---

## ⚙️ Development Stack

- **n8n** (workflow automation with UI) — runs locally via Docker
- **PostgreSQL** — for workflow persistence & future patient lookup
- **Cursor AI** — for source code editing, JSON workflow enhancements, and AI dev assistance
- **Docker Compose** — full local development orchestration
- **Ngrok (optional)** — if external webhook testing is required
- **HTML/React uploader page (optional)** — for drag-and-drop dataset input

---

## 🧠 Guidance for AI (Cursor) — Critical

When writing or modifying code:

> **You are assisting in development of the Staffbotics pipeline.**

### You MUST follow these rules:

- **Do not perform clinical inference or variable extraction — only structural grouping.**
- Group data using identifiers, file structure, folder names, filenames, or deterministic logic.
- Follow core pipeline architecture:
  1. Ingest inputs (Excel, PDFs, folders, etc.)
  2. Detect structure & suggest `GroupingConfig`
  3. Auto grouping into `PatientCandidate[]`
  4. Prompt user for reorganization (natural language allowed)
  5. Apply changes
  6. Emit structured output
- Use and preserve the following JSON data models:

```json
// RawItem
{
  "id": "excel1_row3",
  "source_type": "excel_row" | "file",
  "source_ref": "patients.xlsx#row_3",
  "metadata": { ... }
}
```

```json
// PatientCandidate
{
  "candidate_id": "cand_001",
  "inferred_key": "Excel row 3",
  "raw_items": [...],
  "confidence": 0.96,
  "notes": []
}
```

```json
// GroupingConfig
{
  "unit_strategy": "excel_row" | "subfolder" | "id_in_filename" | "llm_assisted",
  "excel": { "file": "...", "row_is_patient": true, "id_column": "..."},
  "folder": { "use_subfolders_as_patients": true, "id_patterns": ["\d{4}"] }
}
```

🧪 Prefer deterministic logic first (folders, filename patterns, excel ID columns). Only use AI matching for structure suggestion or fallback grouping.

⚠️ Never infer medical content. Only structure and group.

---

## 🚀 Local Development Workflow

### 🟢 Start system

```bash
docker compose up -d
```

Access UI at:  
👉 http://localhost:5678  
(Default credentials if enabled: `admin` / `admin`)

---

## 🔁 Workflow Iteration (with Cursor)

```bash
# Export workflow (from n8n UI to local file)
docker exec n8n n8n export-workflow --id <workflowId> --output /data/workflows/patient-grouping.json

# Edit JSON in Cursor (AI assistance enabled)
cursor .

# After editing
docker exec n8n n8n import-workflow --input /data/workflows/patient-grouping.json
```

Then refresh the UI.

---

## 📂 Testing Data Upload via Drag & Drop

Create a local HTML file (eg. `uploader.html`):

```html
<!DOCTYPE html>
<html>
<body>
  <h3>Upload Dataset</h3>
  <form action="http://localhost:5678/webhook/staffbotics-batch" method="POST" enctype="multipart/form-data">
    <input type="file" name="batchFiles" webkitdirectory directory multiple />
    <button type="submit">Send</button>
  </form>
</body>
</html>
```

Open in browser, drag the folder with test files → submit.

---

## 🧪 Example Input Structure (JSON via REST test)

```json
{
  "excelFiles": [
    {
      "name": "patients.xlsx",
      "rows": [
        { "Name": "John Doe", "NHC": "1234", "Age": 59 },
        { "Name": "Anna Smith", "NHC": "1235", "Age": 71 }
      ]
    }
  ],
  "files": [
    { "path": "batch/1234_report.pdf", "filename": "1234_report.pdf" },
    { "path": "batch/1234_scan.pdf", "filename": "1234_scan.pdf" },
    { "path": "batch/1235_report.pdf", "filename": "1235_report.pdf" }
  ]
}
```

---

## 🔄 Conventions

| Rule | Reason |
|------|--------|
| Do **not** infer clinical data | That is handled by Athena Tech later |
| Always group structurally | Reduces medical errors |
| Prefer deterministic logic | Avoids AI hallucinations |
| Log low-confidence matches | Human validation required |
| Test with real data edge cases | Validate grouping reliability |

---

## 🔮 Future Extensions

- Integrate DB patient existence detection
- Add progress tracking via Staffbotics backend
- Generate automatic import audit reports
- Full UI wrapper (React-based) with conversational corrections

---

## 📝 Recommended Next Steps

1. Build basic workflow in n8n UI
2. Export & improve using Cursor
3. Add grouping logic code into `src/grouping.js`
4. Create test batch upload
5. Iterate based on user review accuracy

---

## 📬 Contact / Team

Lead: **Joan Ficapal Vila** — AI/ML & Survival Analysis  
Company: **Athena Tech & Staffbotics**  
Focus: AI infrastructure for personalized medicine

---

## 📌 Final Reminder to AI Assistants (Cursor)

> *You are here to accelerate development following strict structure logic. Do not generate medical reasoning. Always respect the grouping rules. Aim for robust, scalable execution, not just correct-looking code.*

---

Happy building! 🚀
Error reading /Users/ficapal/LocalProjects/staffbotics_n8n/scripts/.DS_Store: 'utf-8' codec can't decode byte 0x80 in position 3131: invalid start byte

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/scripts/n8n_init.sh
TOTAL LINES: 112
FILE NAME: n8n_init.sh
FILE EXTENSION: .sh
--------------------------------------------------------------------------------
#!/bin/sh
# scripts/n8n_init.sh
#
# Goal (pre-server):
# - Wait for Postgres
# - Trigger n8n migrations (create tables)
# - Import the workflow JSON if missing
# - Ensure exactly one workflow row with WF_NAME is active=true (winner = latest updatedAt)
#
# Important:
# - This does NOT guarantee production webhooks are registered yet.
#   Webhook registration happens inside the running server process.
#   The post-start toggle script handles the #21614-style edge case. :contentReference[oaicite:2]{index=2}
#
# Uses n8n CLI commands:
# - n8n import:workflow
# - n8n update:workflow

set -eu

log() { echo "[$(date -u +'%Y-%m-%dT%H:%M:%SZ')] $*"; }

: "${WF_NAME:?WF_NAME is required (must match workflow JSON name)}"
: "${WF_JSON_PATH:?WF_JSON_PATH is required}"
: "${DB_POSTGRESDB_HOST:?}"
: "${DB_POSTGRESDB_PORT:?}"
: "${DB_POSTGRESDB_DATABASE:?}"
: "${DB_POSTGRESDB_USER:?}"
: "${DB_POSTGRESDB_PASSWORD:?}"

export PGPASSWORD="$DB_POSTGRESDB_PASSWORD"

log "⏳ Waiting for Postgres..."
until pg_isready -h "$DB_POSTGRESDB_HOST" -p "$DB_POSTGRESDB_PORT" -U "$DB_POSTGRESDB_USER" -d "$DB_POSTGRESDB_DATABASE" >/dev/null 2>&1; do
  sleep 1
done
log "✅ Postgres is ready."

# ---- Run migrations by briefly starting n8n ----
log "🧱 Running n8n migrations bootstrap (start/healthz/stop)..."
n8n start >/tmp/n8n-init-start.log 2>&1 &
N8N_PID="$!"

log "⏳ Waiting for n8n /healthz..."
i=0
until curl -fsS "http://127.0.0.1:${N8N_PORT:-5678}/healthz" >/dev/null 2>&1; do
  i=$((i+1))
  if [ "$i" -ge 180 ]; then
    log "❌ n8n did not become healthy in time. Last logs:"
    tail -n 200 /tmp/n8n-init-start.log || true
    kill "$N8N_PID" >/dev/null 2>&1 || true
    exit 1
  fi
  sleep 1
done
log "✅ n8n is healthy. Verifying schema exists..."

# Make sure workflow_entity exists now
psql -h "$DB_POSTGRESDB_HOST" -p "$DB_POSTGRESDB_PORT" -U "$DB_POSTGRESDB_USER" -d "$DB_POSTGRESDB_DATABASE" -tA \
  -c "SELECT 1 FROM workflow_entity LIMIT 1;" >/dev/null 2>&1 || {
    log "❌ workflow_entity still not queryable after migrations bootstrap."
    kill "$N8N_PID" >/dev/null 2>&1 || true
    exit 1
  }

log "✅ Schema present (workflow_entity is queryable)."

log "🛑 Stopping temporary n8n (migrations done)..."
kill "$N8N_PID" >/dev/null 2>&1 || true
wait "$N8N_PID" >/dev/null 2>&1 || true
log "✅ Temporary n8n stopped."

# ---- import if missing ----
WF_ESCAPED="$(printf "%s" "$WF_NAME" | sed "s/'/''/g")"

log "🔎 Checking if workflow exists (name='$WF_NAME')..."
EXISTS_COUNT="$(
  psql -h "$DB_POSTGRESDB_HOST" -p "$DB_POSTGRESDB_PORT" -U "$DB_POSTGRESDB_USER" -d "$DB_POSTGRESDB_DATABASE" -tA \
    -c "SELECT COUNT(*) FROM workflow_entity WHERE name='${WF_ESCAPED}';"
)"

if [ "${EXISTS_COUNT:-0}" = "0" ]; then
  [ -f "$WF_JSON_PATH" ] || { log "❌ Missing workflow JSON at $WF_JSON_PATH"; exit 1; }
  log "⬆️ Importing workflow from: $WF_JSON_PATH"
  n8n import:workflow --input="$WF_JSON_PATH"
  log "✅ Import complete."
else
  log "✅ Workflow already exists ($EXISTS_COUNT row(s)); skipping import."
fi

# ---- choose winner and activate ----
WINNER_ID="$(
  psql -h "$DB_POSTGRESDB_HOST" -p "$DB_POSTGRESDB_PORT" -U "$DB_POSTGRESDB_USER" -d "$DB_POSTGRESDB_DATABASE" -tA \
    -c "SELECT id FROM workflow_entity WHERE name='${WF_ESCAPED}' ORDER BY \"updatedAt\" DESC NULLS LAST LIMIT 1;"
)"

[ -n "$WINNER_ID" ] || { log "❌ Could not find workflow after import/check."; exit 1; }

log "🏁 Winner workflow id: $WINNER_ID"

log "🧹 Deactivating any duplicates with same name..."
psql -h "$DB_POSTGRESDB_HOST" -p "$DB_POSTGRESDB_PORT" -U "$DB_POSTGRESDB_USER" -d "$DB_POSTGRESDB_DATABASE" -v ON_ERROR_STOP=1 \
  -c "UPDATE workflow_entity SET active=false WHERE name='${WF_ESCAPED}';" >/dev/null

log "✅ Activating winner workflow via n8n CLI..."
n8n update:workflow --id="$WINNER_ID" --active=true

log "📌 Current rows for '$WF_NAME':"
psql -h "$DB_POSTGRESDB_HOST" -p "$DB_POSTGRESDB_PORT" -U "$DB_POSTGRESDB_USER" -d "$DB_POSTGRESDB_DATABASE" \
  -c "SELECT id, name, active, \"updatedAt\" FROM workflow_entity WHERE name='${WF_ESCAPED}';" || true

log "✅ Init completed successfully."

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/scripts/n8n_poststart_toggle.sh
TOTAL LINES: 179
FILE NAME: n8n_poststart_toggle.sh
FILE EXTENSION: .sh
--------------------------------------------------------------------------------
#!/bin/sh
# scripts/n8n_poststart_toggle.sh
#
# PURPOSE
# -------
# Work around an n8n failure mode where:
# - The workflow is active in the DB (active=true)
# - n8n logs may even say "Activated workflow ..."
# BUT:
# - Production webhook URL /webhook/<path> returns:
#     {"code":404,"message":"The requested webhook \"POST <path>\" is not registered.", ...}
#
# WHY THIS HAPPENS (PRACTICAL VIEW)
# -------------------------------
# Webhook routes are registered INSIDE the running n8n server process.
# If something goes wrong during startup registration, you can end up with:
# - DB says active=true
# - server didn't register the route
#
# WHAT THIS SCRIPT DOES
# ---------------------
# 1) Wait until the main n8n server is healthy (/healthz)
# 2) Lookup workflow ID by WF_NAME directly in Postgres (winner = latest updatedAt)
# 3) Toggle activation "off then on" using the same REST endpoint the editor uses:
#      PATCH /rest/workflows/:id  {"active":false}
#      PATCH /rest/workflows/:id  {"active":true}
#    This forces the running server to re-register the webhooks.
# 4) Optionally verify the webhook no longer returns the "not registered" 404
#
# REQUIREMENTS
# ------------
# Inside the container image:
# - curl
# - psql client
#
# Required environment variables:
# - WF_NAME
# - DB_POSTGRESDB_HOST, DB_POSTGRESDB_PORT, DB_POSTGRESDB_DATABASE, DB_POSTGRESDB_USER, DB_POSTGRESDB_PASSWORD
# - N8N_INTERNAL_URL (e.g. http://n8n:5678)
#
# Optional environment variables:
# - N8N_BASIC_AUTH_USER, N8N_BASIC_AUTH_PASSWORD  (if /rest/* is protected by basic auth)
# - WEBHOOK_METHOD (default POST)
# - WEBHOOK_PATH (default staffbotics-batch)
#
# Exit codes:
# - 0 success (toggle done, webhook probe not "not registered")
# - 1 failure (can't reach health, can't find workflow, rest patch fails, still not registered)

set -eu

log() { echo "[$(date -u +'%Y-%m-%dT%H:%M:%SZ')] $*"; }

# -----------------------------
# Validate required env
# -----------------------------
: "${WF_NAME:?WF_NAME is required}"
: "${DB_POSTGRESDB_HOST:?DB_POSTGRESDB_HOST is required}"
: "${DB_POSTGRESDB_PORT:?DB_POSTGRESDB_PORT is required}"
: "${DB_POSTGRESDB_DATABASE:?DB_POSTGRESDB_DATABASE is required}"
: "${DB_POSTGRESDB_USER:?DB_POSTGRESDB_USER is required}"
: "${DB_POSTGRESDB_PASSWORD:?DB_POSTGRESDB_PASSWORD is required}"
: "${N8N_INTERNAL_URL:?N8N_INTERNAL_URL is required (e.g. http://n8n:5678)}"

export PGPASSWORD="$DB_POSTGRESDB_PASSWORD"

HEALTH_URL="${N8N_INTERNAL_URL%/}/healthz"

WEBHOOK_METHOD="${WEBHOOK_METHOD:-POST}"
WEBHOOK_PATH="${WEBHOOK_PATH:-staffbotics-batch}"
WEBHOOK_URL="${N8N_INTERNAL_URL%/}/webhook/${WEBHOOK_PATH}"

# If basic auth is set in compose, use it for /rest/* calls.
# (If your instance does not use basic auth, this becomes a no-op.)
BASIC_AUTH_ARGS=""
if [ -n "${N8N_BASIC_AUTH_USER:-}" ] && [ -n "${N8N_BASIC_AUTH_PASSWORD:-}" ]; then
  BASIC_AUTH_ARGS="-u ${N8N_BASIC_AUTH_USER}:${N8N_BASIC_AUTH_PASSWORD}"
fi

# -----------------------------
# 1) Wait for n8n health
# -----------------------------
log "⏳ Waiting for n8n healthz at: $HEALTH_URL"
i=0
until curl -fsS "$HEALTH_URL" >/dev/null 2>&1; do
  i=$((i+1))
  if [ "$i" -ge 180 ]; then
    log "❌ n8n did not become healthy in time."
    exit 1
  fi
  sleep 1
done
log "✅ n8n is healthy."

# -----------------------------
# 2) Lookup workflow id (winner)
# -----------------------------
WF_ESCAPED="$(printf "%s" "$WF_NAME" | sed "s/'/''/g")"

WF_ID="$(
  psql -h "$DB_POSTGRESDB_HOST" -p "$DB_POSTGRESDB_PORT" -U "$DB_POSTGRESDB_USER" -d "$DB_POSTGRESDB_DATABASE" -tA \
    -c "SELECT id
        FROM workflow_entity
        WHERE name='${WF_ESCAPED}'
        ORDER BY \"updatedAt\" DESC NULLS LAST
        LIMIT 1;"
)"

if [ -z "${WF_ID:-}" ]; then
  log "❌ Could not find workflow in DB by name='$WF_NAME'"
  exit 1
fi

log "🏁 Target workflow id: $WF_ID (name='$WF_NAME')"

# -----------------------------
# 3) Toggle active off/on via REST
# -----------------------------
REST_URL="${N8N_INTERNAL_URL%/}/rest/workflows/${WF_ID}"

rest_patch() {
  # Args:
  # - $1 workflow id
  # - $2 json payload
  id="$1"
  json="$2"
  url="${N8N_INTERNAL_URL%/}/rest/workflows/${id}"

  # -f: fail on HTTP >= 400
  # -sS: silent but show errors
  # BASIC_AUTH_ARGS included if defined
  curl -fsS \
    $BASIC_AUTH_ARGS \
    -X PATCH \
    -H "Content-Type: application/json" \
    -d "$json" \
    "$url" \
    >/dev/null
}

log "🔁 Toggling active=false via $REST_URL ..."
rest_patch "$WF_ID" '{"active":false}'

# Small pause so n8n processes the deactivation
sleep 1

log "🔁 Toggling active=true via $REST_URL ..."
rest_patch "$WF_ID" '{"active":true}'

log "✅ Toggled workflow via REST endpoint."

# -----------------------------
# 4) Verify webhook is registered
# -----------------------------
# We only verify that we are NOT getting the specific "not registered" 404.
# Your workflow may still return other errors depending on responseMode,
# missing nodes, auth, etc. That is separate from webhook registration.
log "🔎 Verifying webhook route with ${WEBHOOK_METHOD} ${WEBHOOK_URL}"

probe_body="$(curl -sS --max-time 6 \
  -X "$WEBHOOK_METHOD" \
  -H "Content-Type: application/json" \
  -d '{}' \
  -w "\n%{http_code}" \
  "$WEBHOOK_URL" || true
)"

http_code="$(printf "%s" "$probe_body" | tail -n 1)"
resp="$(printf "%s" "$probe_body" | sed '$d')"

if [ "$http_code" = "404" ] && printf "%s" "$resp" | grep -qi "not registered"; then
  log "❌ Still seeing 'not registered' for ${WEBHOOK_METHOD} ${WEBHOOK_PATH}"
  log "Response was: $resp"
  exit 1
fi

log "✅ Webhook does not return the 'not registered' 404 anymore."
log "Done."
exit 0

--------------------------------------------------------------------------------
NEW FILE: /Users/ficapal/LocalProjects/staffbotics_n8n/scripts/reset_db.sh
TOTAL LINES: 0
FILE NAME: reset_db.sh
FILE EXTENSION: .sh
--------------------------------------------------------------------------------
